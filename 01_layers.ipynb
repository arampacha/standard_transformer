{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from fastai.basics import *\n",
    "\n",
    "from functools import partial, reduce\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "try:\n",
    "    from axial_positional_embedding import AxialPositionalEmbedding, AxialPositionalEmbeddingImage\n",
    "except ImportError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def expand_dim1(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return x[None, :]\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Residual(Module):\n",
    "    \"\"\"Add skip-connection: out = x + sublayer(x)\"\"\"\n",
    "    def __init__(self, sublayer:Module): store_attr()\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PostNorm(Module):\n",
    "    \"\"\"Adds LayerNorm after sublayer\"\"\"\n",
    "    def __init__(self, d_model:int, sublayer:Module):\n",
    "        store_attr('sublayer')\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.sublayer(x, *args, **kwargs)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class PreNorm(Module):\n",
    "    \"\"\"Adds LayerNorm before sublayer\"\"\"\n",
    "    def __init__(self, d_model:int, sublayer:Module):\n",
    "        store_attr('sublayer')\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeedForward(Module):\n",
    "    \"\"\"\n",
    "    Simple positional feed-forward module with GELU activation function.\n",
    "    If d_ff is None defaults to 4*d_model\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, d_ff:int=None, dropout:float=0.):\n",
    "        d_ff = default(d_ff, 4 * d_model)\n",
    "        layers = [nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model), nn.Dropout(dropout)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(p) for p in self.parameters() if p.dim() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "ff  = Residual(PreNorm(d, FeedForward(d)))\n",
    "out = ff(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MASK_VAL = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttnInProj(Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = ifnone(context, x)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_k(context), self.to_v(context)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-16, d)\n",
    "proj = AttnInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 112, 64]), torch.Size([4, 112, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == context.size()\n",
    "assert all_equal(q1, q2)\n",
    "assert not all_equal(k1, k2)\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO make sure store_attention works\n",
    "class ScaledDotProdAttention(Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, causal=False, dropout=0., store_attention:bool=False):\n",
    "        store_attr()\n",
    "        self.scale = (d_model//n_heads)**-0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None, context_mask=None):\n",
    "        device = q.device\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_heads), (q, k, v))\n",
    "        \n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            \n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            input_mask = q_mask * k_mask\n",
    "        \n",
    "        # classic dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q*self.scale, k)\n",
    "        \n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n",
    "            dots.masked_fill_(mask, MASK_VAL)\n",
    "            del mask\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: self.attention = attn.detach().cpu()\n",
    "        \n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(bs, sl, d)\n",
    "k = torch.randn(bs, sl, d)\n",
    "v = torch.randn(bs, sl, d)\n",
    "attn_func = ScaledDotProdAttention(d, 4)\n",
    "out = attn_func(q, k, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(Module):\n",
    "    \"\"\"\n",
    "    Standard attention module using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=True,\n",
    "                 store_attention:bool=False):\n",
    "        store_attr('causal, mask, n_heads, bias')\n",
    "        out_dropout = ifnone(out_dropout, dropout)\n",
    "        self.in_proj = AttnInProj(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        q, k, v = self.in_proj(x, context)\n",
    "        \n",
    "        out = self.attn(q, k, v, mask, context_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "attn  = Residual(PreNorm(d, Attention(d)))\n",
    "out = attn(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO test with context, masks\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-20, d)\n",
    "attn  = Residual(PreNorm(d, Attention(d)))\n",
    "\n",
    "out = attn(x, context)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder attention class combining self and cross attention \n",
    "# may be replaced with generalized attention in future\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 heads = 8, \n",
    "                 causal = False,\n",
    "                 mask = None,\n",
    "                 dropout=0.1, \n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.store_attention = False\n",
    "        self.mask = mask #??\n",
    "        self.heads = heads\n",
    "        self.scale = (d_model//heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(d_model, d_model, bias = bias)\n",
    "        self.to_kv = nn.Linear(d_model, d_model * 2, bias = bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None, store_attention=False):\n",
    "        b, n, d, h, device = *x.shape, self.heads, x.device\n",
    "        context = default(context, torch.empty(b, 0, d, dtype=x.dtype, device=device))\n",
    "        kv_input = torch.cat([x, context], dim=-2)\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
    "\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            self_mask = q_mask[:, None, :, None] * q_mask[:, None, None, :]\n",
    "            if context.size(-2) != 0:\n",
    "                k_mask = default(context_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "                cross_mask = q_mask[:, None, :, None] * k_mask[:, None, None, :]\n",
    "            else: cross_mask = torch.empty(0, dtype=self_mask.dtype, device=device)\n",
    "            input_mask = torch.cat([self_mask, cross_mask], dim=-1)\n",
    "        \n",
    "        # classic scaled dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q * self.scale, k)\n",
    "        \n",
    "        # might need to tune MASK_VAL for fp16 to work\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(n, n, 1)\n",
    "            dots[:,:,i,j] = MASK_VAL\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: # and not self.training\n",
    "            self.attention = attn.detach().cpu()\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in [self.to_q.weight, self.to_kv.weight, self.to_out.weight]]\n",
    "        if getattr(self.to_q, 'bias', None) is not None: nn.init.constant_(self.to_q.bias, 0)\n",
    "        if getattr(self.to_kv, 'bias', None) is not None: nn.init.constant_(self.to_kv.bias, 0)\n",
    "        nn.init.constant_(self.to_out.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderBlock(Module):\n",
    "    \"\"\"\n",
    "    Bacis transformer encoder block. Consists of multi-head attention and positional \n",
    "    feedforward layers\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_heads = 8, \n",
    "                 d_ff = None, \n",
    "                 attn_dropout = 0.1,\n",
    "                 ff_dropout = 0.1,\n",
    "                 causal = False, \n",
    "                 mask = None, \n",
    "                 attn_bias = True, \n",
    "                 prenorm=False):\n",
    "        store_attr('attn_dropout') # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(d_model, Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = Residual(PreNorm(d_model, FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(d_model, Residual(Attention(d_model, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = PostNorm(d_model, Residual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None): #? more args\n",
    "        out = self.attn(x, mask=mask)\n",
    "        out = self.dropout(out)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderBlock(d)\n",
    "out = m(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoder(Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None,\n",
    "                 ff_dropout=0.1, \n",
    "                 attn_dropout=0.1,\n",
    "                 attn_bias=True,\n",
    "                 causal=False, \n",
    "                 prenorm=False, \n",
    "                 final_norm=None):\n",
    "        store_attr('d_model')\n",
    "        self.layers = nn.ModuleList([])    \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(TransformerEncoderBlock(d_model, n_heads, causal=causal, \n",
    "                                    d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                    prenorm=prenorm, attn_bias=attn_bias))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers: x = layer(x, mask=mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoder(d)\n",
    "out = m(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerDecoderBlock(Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_heads = 8, \n",
    "                 d_ff = None,\n",
    "                 attn_dropout = 0.1, \n",
    "                 ff_dropout=0.1,\n",
    "                 mask = None ,\n",
    "                 attn_bias = True,\n",
    "                 prenorm=False):\n",
    "        store_attr('attn_dropout')     # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(d_model, Attention(d_model, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.cross = Residual(PreNorm(d_model, Attention(d_model, n_heads=n_heads, causal=False, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = Residual(PreNorm(d_model, FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(d_model, Residual(Attention(d_model, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.cross = PostNorm(d_model, Residual(Attention(d_model, n_heads=n_heads, causal=False, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = PostNorm(d_model, Residual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        out = self.dropout(out)\n",
    "        out = self.cross(out, context, mask=mask, context_mask=context_mask)\n",
    "        out = self.dropout(out)\n",
    "        return self.ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerDecoderBlockV2(nn.Module):\n",
    "    def __init__(self, d_model, n_heads = 8, mask = None, d_ff=None,\n",
    "                 attn_dropout=0.1, ff_dropout=0.1, attn_bias=True,\n",
    "                 prenorm=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = attn_dropout # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(d_model, AdditiveAttention(d_model, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = Residual(PreNorm(d_model, FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(d_model, Residual(AdditiveAttention(d_model, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = PostNorm(d_model, Residual(FeedForward(d_model, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        out = self.attn(x, context, mask=mask, context_mask=context_mask)\n",
    "        out = F.dropout(out, p=self.attn_dropout)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "class TransformerDecoder(Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_layers=6, \n",
    "                 n_heads=8, \n",
    "                 d_ff=None, \n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1, \n",
    "                 prenorm=False, \n",
    "                 comb_attn=False, \n",
    "                 attn_bias=True, \n",
    "                 final_norm=None):\n",
    "        store_attr('d_model')\n",
    "        #TODO(Arto) refactor\n",
    "        block = TransformerDecoderBlockV2 if comb_attn else TransformerDecoderBlock\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(block(d_model, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, \n",
    "                                     ff_dropout=ff_dropout, prenorm=prenorm, attn_bias=attn_bias))\n",
    "        self.norm = None if final_norm is None else final_norm(d_model)\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        for layer in self.layers: x = layer(x, context, mask, context_mask)\n",
    "        if self.norm is not None: x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AbsolutePositionalEmbedding(Module):\n",
    "    \"\"\"Learnable absolute positional encodings\"\"\"\n",
    "    def __init__(self, d_emb:int, max_seq_len:int):\n",
    "        self.emb = nn.Embedding(max_seq_len, d_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FixedPositionalEmbedding(Module):\n",
    "    \"\"\"Fixed positional encodings\"\"\"\n",
    "    def __init__(self, d_emb:int):\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, d_emb, 2).float() / d_emb))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEmbedding(Module):\n",
    "    \"\"\"\n",
    "    Combines token embedings with positional encodings\n",
    "    pos_enc: str from {'absolute', 'fixed', 'axial'}\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 emb_sz:int, \n",
    "                 d_emb:int, \n",
    "                 max_seq_len:int=512, \n",
    "                 dropout:float=0., \n",
    "                 pos_enc:str='absolute', \n",
    "                 axial_shape:Tuple=None, \n",
    "                 axial_emb_dims:Tuple=None):\n",
    "        store_attr('d_emb')\n",
    "        self.scale = d_emb ** 0.5\n",
    "        self.std = 0.02    # fairseq: d_emb ** -0.5, fastai: 0.01\n",
    "        self.emb = nn.Embedding(emb_sz, d_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if pos_enc == 'absolute': self.pos_enc = AbsolutePositionalEmbedding(d_emb, max_seq_len)\n",
    "        elif pos_enc == 'fixed': self.pos_enc = FixedPositionalEmbedding(d_emb)\n",
    "        elif pos_enc == 'axial':\n",
    "            assert axial_shape is not None\n",
    "            assert reduce(mul, axial_shape) == max_seq_len\n",
    "            axial_emb_dims = default(axial_emb_dims, get_axial_dims(d_emb, len(axial_shape)))\n",
    "            self.pos_enc = AxialPositionalEmbedding(d_emb, axial_shape, axial_emb_dims)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)  #* self.scale\n",
    "        x *= self.scale \n",
    "        x += self.pos_enc(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    def _init(self):\n",
    "        nn.init.trunc_normal_(self.emb.weight, std = self.std)\n",
    "        if hasattr(self.pos_enc, 'weight'): nn.init.trunc_normal_(self.pos_enc.weight, std = self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv] *",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
