{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial, reduce\n",
    "from inspect import isfunction\n",
    "from operator import mul\n",
    "from fastai.basics import *\n",
    "from einops import rearrange, repeat\n",
    "try:\n",
    "    from axial_positional_embedding import AxialPositionalEmbedding, AxialPositionalEmbeddingImage\n",
    "except ImportError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def expand_dim1(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return x[None, :]\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Residual(nn.Module):\n",
    "    \"\"\"Add skip-connection: out = x + sublayer(x)\"\"\"\n",
    "    def __init__(self, sublayer):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.sublayer(x, *args, **kwargs) + x\n",
    "    \n",
    "class PostNorm(nn.Module):\n",
    "    \"\"\"Adds LayerNorm after sublayer\"\"\"\n",
    "    def __init__(self, dim, sublayer):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.sublayer(x, *args, **kwargs)\n",
    "        return self.norm(x)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"Adds LayerNorm before sublayer\"\"\"\n",
    "    def __init__(self, dim, sublayer):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.sublayer(x, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple positional feed-forward module with GELU activation function.\n",
    "    If d_ff is None defaults to 4*d_model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, d_ff=None, dropout=0.):\n",
    "        super().__init__()\n",
    "        d_ff = default(d_ff, 4*dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self._init()\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    def _init(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim()>1: nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "ff  = Residual(PreNorm(d, FeedForward(d)))\n",
    "out = ff(x)\n",
    "assert (bs, sl, d) == out.size()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MASK_VAL = -5e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Standard attention module\"\"\"\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 n_heads = 8, \n",
    "                 causal = False,\n",
    "                 mask = None,\n",
    "                 dropout=0.1,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.store_attention = False\n",
    "        self.mask = mask #??\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim//n_heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(dim, dim, bias=bias)\n",
    "        self.to_kv = nn.Linear(dim, dim * 2, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None, store_attention=False):\n",
    "        b, n, _, h, device = *x.shape, self.n_heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            input_mask = q_mask * k_mask\n",
    "        # classic dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q*self.scale, k)\n",
    "        # might need to tune MASK_VAL for fp16 to work\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n",
    "            dots.masked_fill_(mask, MASK_VAL)\n",
    "            del mask\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: # and not self.training\n",
    "            self.attention = attn.detach().cpu()\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        #out = self.dropout(out) # option for more dropout here\n",
    "        return out\n",
    "    \n",
    "    def _init(self):\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_kv.weight)\n",
    "        nn.init.xavier_uniform_(self.to_out.weight)\n",
    "        if getattr(self.to_q, 'bias', None) is not None:\n",
    "            nn.init.constant_(self.to_q.bias, 0)\n",
    "        if getattr(self.to_kv, 'bias', None) is not None:\n",
    "            nn.init.constant_(self.to_kv.bias, 0)\n",
    "        nn.init.constant_(self.to_out.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Additive attention combining self and cross attention\"\"\"\n",
    "    def __init__(self, \n",
    "                 dim, \n",
    "                 n_heads = 8, \n",
    "                 causal = False,\n",
    "                 mask = None,\n",
    "                 dropout=0.1, \n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.store_attention = False\n",
    "        self.mask = mask #??\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim//n_heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(dim, dim, bias = bias)\n",
    "        self.to_kv = nn.Linear(dim, dim * 2, bias = bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None, store_attention=False):\n",
    "        b, n, d, h, device = *x.shape, self.n_heads, x.device\n",
    "        context = default(context, torch.empty(b, 0, d, dtype=x.dtype, device=device))\n",
    "        kv_input = torch.cat([x, context], dim=-2)\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
    "\n",
    "        # boolean input_mask is False at positions not to attend to\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            self_mask = q_mask[:, None, :, None] * q_mask[:, None, None, :]\n",
    "            if context.size(-2) != 0:\n",
    "                k_mask = default(context_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "                cross_mask = q_mask[:, None, :, None] * k_mask[:, None, None, :]\n",
    "            else: cross_mask = torch.empty(0, dtype=self_mask.dtype, device=device)\n",
    "            input_mask = torch.cat([self_mask, cross_mask], dim=-1)\n",
    "        # classic scaled dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q * self.scale, k)\n",
    "        # might need to tune MASK_VAL for fp16 to work\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(n, n, 1)\n",
    "            dots[:,:,i,j] = MASK_VAL\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: # and not self.training\n",
    "            self.attention = attn.detach().cpu()\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_kv.weight)\n",
    "        nn.init.xavier_uniform_(self.to_out.weight)\n",
    "        if getattr(self.to_q, 'bias', None) is not None:\n",
    "            nn.init.constant_(self.to_q.bias, 0)\n",
    "        if getattr(self.to_kv, 'bias', None) is not None:\n",
    "            nn.init.constant_(self.to_kv.bias, 0)\n",
    "        nn.init.constant_(self.to_out.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AttnInProj(nn.Module):\n",
    "    \"\"\"Computes q, k, v from input x and [optional] context\"\"\"\n",
    "    def __init__(self, d_model:int, bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.to_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.to_kv = nn.Linear(d_model, 2*d_model, bias=bias)\n",
    "    def forward(self, x, context=None):\n",
    "        context = default(context, x)\n",
    "        q = self.to_q(x)\n",
    "        k, v = self.to_kv(context).chunk(2, -1)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl, d)\n",
    "proj = AttnInProj(d)\n",
    "q1, k1, v1 = proj(x)\n",
    "assert (bs, sl, d) == q1.size() == k1.size() == v1.size()\n",
    "q1.shape, k1.shape, v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 64]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2, k2, v2 = proj(x, context)\n",
    "assert (bs, sl, d) == q2.size()\n",
    "assert k2.size() == v2.size() == context.size()\n",
    "assert (q1==q2).all()\n",
    "assert not (k1==k2).all()\n",
    "q2.shape, k2.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO make sure store_attention works\n",
    "class ScaledDotProdAttention(Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, causal=False, dropout=0., store_attention:bool=False):\n",
    "        store_attr()\n",
    "        self.scale = (d_model//n_heads)**-0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        device = q.device\n",
    "        bs, sl, d, cl = *q.size(), k.size(1)\n",
    "        \n",
    "        q = q.view(bs, sl, self.n_heads, -1).transpose(1, 2)\n",
    "        k = k.view(bs, cl, self.n_heads, -1).transpose(1, 2)\n",
    "        v = v.view(bs, cl, self.n_heads, -1).transpose(1, 2)\n",
    "        # classic dot-product attention\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q*self.scale, k)\n",
    "        \n",
    "        if exists(attn_mask):\n",
    "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
    "            del input_mask\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(sl, sl, 1)\n",
    "            dots[:,:,i,j] = MASK_VAL\n",
    "\n",
    "        attn = F.softmax(dots, -1)\n",
    "        if self.store_attention: self.attention = attn.detach().cpu()\n",
    "        \n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.einsum('bhij, bhjd -> bihd', attn, v)\n",
    "        return out.contiguous().view(bs, sl, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot-product attention is calculated as:\n",
    "\n",
    "$$\\textbf {Attention}(Q,K,V) = \\textbf {softmax}({QK^T\\over\\sqrt d_k})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(bs, sl, d)\n",
    "k = torch.randn(bs, sl, d)\n",
    "v = torch.randn(bs, sl, d)\n",
    "attn_func = ScaledDotProdAttention(d, 4)\n",
    "out = attn_func(q, k, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard attention module using scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 n_heads:int = 8, \n",
    "                 causal:bool = False,\n",
    "                 mask:Tensor = None,\n",
    "                 dropout:float=0.1,\n",
    "                 out_dropout:float=None,\n",
    "                 bias:bool=False,\n",
    "                 store_attention:bool=False):\n",
    "        super().__init__()\n",
    "        store_attr('causal, mask, n_heads, bias')\n",
    "        out_dropout = default(out_dropout, dropout)\n",
    "        self.in_proj = AttnInProj(d_model, bias=bias)\n",
    "        self.attn = ScaledDotProdAttention(d_model, n_heads, causal=causal,\n",
    "                                           dropout=dropout, store_attention=store_attention)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(out_dropout)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        q, k, v = self.in_proj(x, context)\n",
    "        \n",
    "        attn_mask = self._make_input_mask(mask, context_mask, x, context)\n",
    "        out = self.attn(q, k, v, attn_mask)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    def _init(self):\n",
    "        [nn.init.xavier_uniform_(w) for w in self.parameters() if w.dim()>1]\n",
    "        if self.bias:\n",
    "            [nn.init.constant_(b, 0) for b in self.parameters() if b.dim()==1]\n",
    "    \n",
    "    def _make_input_mask(self, mask, context_mask, x, context):\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            b, n, _, device = *x.size(), x.device\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, context.shape[-2]), device = device).bool())\n",
    "            \n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            return q_mask * k_mask\n",
    "        else: return None #input_mask is None if both mask and context_mask are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "attn  = Residual(PreNorm(d, Attention(d)))\n",
    "out = attn(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO test with context, masks\n",
    "x = torch.randn(bs, sl, d)\n",
    "context = torch.randn(bs, sl-20, d)\n",
    "attn  = Residual(PreNorm(d, Attention(d)))\n",
    "\n",
    "out = attn(x, context)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bacis transformer encoder block. Consists of multi-head attention and positional feedforward layers\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads = 8, causal = False, mask = None, \n",
    "                 attn_dropout=0.1, attn_bias=True, ff_dropout=0.1, d_ff=None, \n",
    "                 prenorm=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = attn_dropout # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(dim, Attention(dim, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = Residual(PreNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(dim, Residual(Attention(dim, n_heads=n_heads, causal=causal, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = PostNorm(dim, Residual(FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        \n",
    "    def forward(self, x, mask=None): #? more args\n",
    "        out = self.attn(x, mask=mask)\n",
    "        out = F.dropout(out, p=self.attn_dropout, training=self.training)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoderBlock(d)\n",
    "out = m(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, depth=6, n_heads=8, causal=False, d_ff=None, attn_dropout=0.1, attn_bias=True,\n",
    "                ff_dropout=0.1, prenorm=False, final_norm=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(TransformerEncoderBlock(dim, n_heads, causal=causal, d_ff=d_ff, \n",
    "                                    attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm, attn_bias=attn_bias))\n",
    "        self.norm = None if final_norm is None else final_norm(dim)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, sl, d)\n",
    "m = TransformerEncoder(d)\n",
    "out = m(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads = 8, mask = None, d_ff=None,\n",
    "                 attn_dropout=0.1, ff_dropout=0.1, attn_bias=True,\n",
    "                 prenorm=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = attn_dropout # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(dim, Attention(dim, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.cross = Residual(PreNorm(dim, Attention(dim, n_heads=n_heads, causal=False, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = Residual(PreNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(dim, Residual(Attention(dim, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.cross = PostNorm(dim, Residual(Attention(dim, n_heads=n_heads, causal=False, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = PostNorm(dim, Residual(FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        out = self.attn(x, mask=mask)\n",
    "        out = F.dropout(out, p=self.attn_dropout, training=self.training)\n",
    "        out = self.cross(out, context, mask=mask, context_mask=context_mask)\n",
    "        out = F.dropout(out, p=self.attn_dropout, training=self.training)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerDecoderBlockV2(nn.Module):\n",
    "    def __init__(self, dim, n_heads = 8, mask = None, d_ff=None,\n",
    "                 attn_dropout=0.1, ff_dropout=0.1, attn_bias=True,\n",
    "                 prenorm=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = attn_dropout # mb separate argument attn_post_dropout\n",
    "        if prenorm:\n",
    "            self.attn = Residual(PreNorm(dim, DecoderAttention(dim, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = Residual(PreNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        else:\n",
    "            self.attn = PostNorm(dim, Residual(DecoderAttention(dim, n_heads=n_heads, causal=True, dropout=attn_dropout, bias=attn_bias)))\n",
    "            self.ff = PostNorm(dim, Residual(FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
    "        \n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        out = self.attn(x, context, mask=mask, context_mask=context_mask)\n",
    "        out = F.dropout(out, p=self.attn_dropout, training=self.training)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, dim, depth=6, n_heads=8, d_ff=None, attn_dropout=0.1, ff_dropout=0.1, \n",
    "                 prenorm=False, comb_attn=False, attn_bias=True, final_norm=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.layers = nn.ModuleList([])\n",
    "        block = TransformerDecoderBlockV2 if comb_attn else TransformerDecoderBlock\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(block(dim, n_heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm, attn_bias=attn_bias))\n",
    "        self.norm = None if final_norm is None else final_norm(dim)\n",
    "    def forward(self, x, context, mask=None, context_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, context, mask, context_mask)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device)\n",
    "        return self.emb(t)\n",
    "\n",
    "class FixedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines token embedings with positional encodings\n",
    "    pos_enc: str from {'absolute', 'fixed', 'axial'}\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_sz, dim, max_seq_len=512, dropout=0., pos_enc='absolute', \n",
    "                 axial_shape=None, axial_emb_dims=None):\n",
    "        super().__init__()\n",
    "        self.scale = dim**0.5\n",
    "        self.emb = nn.Embedding(emb_sz, dim)\n",
    "        if pos_enc == 'absolute':\n",
    "            self.pos_enc = AbsolutePositionalEmbedding(dim, max_seq_len)\n",
    "        elif pos_enc == 'fixed':\n",
    "            self.pos_enc = FixedPositionalEmbedding(dim)\n",
    "        elif pos_enc == 'axial':\n",
    "            assert axial_shape is not None\n",
    "            assert reduce(mul, axial_shape) == max_seq_len\n",
    "            axial_emb_dims = default(axial_emb_dims, get_axial_dims(dim, len(axial_shape)))\n",
    "            self.pos_enc = AxialPositionalEmbedding(dim, axial_shape, axial_emb_dims)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._init()\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x *= self.scale\n",
    "        x += self.pos_enc(x)\n",
    "        return self.dropout(x)\n",
    "    def _init(self):\n",
    "        nn.init.trunc_normal_(self.emb.weight, std=1/self.scale)\n",
    "        # 0.02 works worse then std=1 for pe, trying d_emb**-0.5\n",
    "#         if hasattr(self.pos_enc, 'emb'):\n",
    "#             nn.init.trunc_normal_(self.pos_enc.emb.weight, std=1/self.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = 256\n",
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "emb = TransformerEmbedding(vocab_sz, d)\n",
    "out = emb(x)\n",
    "assert (bs, sl, d) == out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = 256\n",
    "bs = 4\n",
    "sl = 128\n",
    "d = 64\n",
    "x = torch.randint(vocab_sz, (bs, sl))\n",
    "emb = TransformerEmbedding(vocab_sz, d, pos_enc='fixed')\n",
    "out = emb(x)\n",
    "assert (bs, sl, d) == out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# class AbsolutePositionalEmbedding(Module):\n",
    "#     \"\"\"Learnable absolute positional encodings\"\"\"\n",
    "#     def __init__(self, d_emb:int, max_seq_len:int):\n",
    "#         self.emb = nn.Embedding(max_seq_len, d_emb)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         t = torch.arange(x.shape[1], device=x.device)\n",
    "#         return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# class FixedPositionalEmbedding(Module):\n",
    "#     \"\"\"Fixed positional encodings\"\"\"\n",
    "#     def __init__(self, d_emb:int):\n",
    "#         inv_freq = 1. / (10000 ** (torch.arange(0, d_emb, 2).float() / d_emb))\n",
    "#         self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "#         sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "#         emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "#         return emb[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# class TransformerEmbedding(Module):\n",
    "#     \"\"\"\n",
    "#     Combines token embedings with positional encodings\n",
    "#     pos_enc: str from {'absolute', 'fixed', 'axial'}\n",
    "#     \"\"\"\n",
    "#     def __init__(self, \n",
    "#                  emb_sz:int, \n",
    "#                  d_emb:int, \n",
    "#                  max_seq_len:int=512, \n",
    "#                  dropout:float=0., \n",
    "#                  pos_enc:str='absolute', \n",
    "#                  axial_shape:Tuple=None, \n",
    "#                  axial_emb_dims:Tuple=None):\n",
    "#         store_attr('d_emb')\n",
    "#         self.scale = d_emb ** 0.5\n",
    "#         self.std = 0.02    # fairseq: d_emb ** -0.5, fastai: 0.01\n",
    "#         self.emb = nn.Embedding(emb_sz, d_emb)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         if pos_enc == 'absolute': self.pos_enc = AbsolutePositionalEmbedding(d_emb, max_seq_len)\n",
    "#         elif pos_enc == 'fixed': self.pos_enc = FixedPositionalEmbedding(d_emb)\n",
    "#         elif pos_enc == 'axial':\n",
    "#             assert axial_shape is not None\n",
    "#             assert reduce(mul, axial_shape) == max_seq_len\n",
    "#             axial_emb_dims = default(axial_emb_dims, get_axial_dims(d_emb, len(axial_shape)))\n",
    "#             self.pos_enc = AxialPositionalEmbedding(d_emb, axial_shape, axial_emb_dims)\n",
    "#         self._init()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.emb(x)  #* self.scale\n",
    "#         x *= self.scale \n",
    "#         x += self.pos_enc(x)\n",
    "#         return self.dropout(x)\n",
    "    \n",
    "#     def _init(self):\n",
    "#         nn.init.trunc_normal_(self.emb.weight, std = self.std)\n",
    "#         if hasattr(self.pos_enc, 'weight'): nn.init.trunc_normal_(self.pos_enc.weight, std = self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
