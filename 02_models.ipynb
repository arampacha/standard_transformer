{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.basics import *\n",
    "from standard_transformer.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# generative helpers\n",
    "# credit https://github.com/huggingface/transformers/blob/a0c62d249303a68f5336e3f9a96ecf9241d7abbe/src/transformers/generation_logits_process.py\n",
    "def top_p_filter(logits, top_p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cum_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    # if min_tokens_to_keep > 1:\n",
    "    #         # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "    #         sorted_indices_to_remove[..., : min_tokens_to_keep - 1] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "def top_k_filter(logits, top_k=20):\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "_sampler = {\n",
    "    'top_k':top_k_filter,\n",
    "    'top_p':top_p_filter,\n",
    "    'gready':lambda x: x.argmax(-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerLM(Module):\n",
    "    \"\"\"\n",
    "    Basic Transformer for language modelling\n",
    "    Parameters:\n",
    "        * vocab_sz: int\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_layers: int (default: 6) \n",
    "        * heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * causal: bool (default: True) - if True does causal masking automatically\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * x - input ids, shape [bs, sl]\n",
    "        * mask - optional boolean mask, shape [bs, sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_sz, \n",
    "                 d_model, \n",
    "                 n_layers=6,\n",
    "                 heads=8,\n",
    "                 d_ff=None,\n",
    "                 attn_dropout=0.1,\n",
    "                 ff_dropout=0.1,\n",
    "                 emb_dropout=0.1,\n",
    "                 tie_weights=True,\n",
    "                 causal=True,\n",
    "                 pos_enc='absolute',\n",
    "                 max_seq_len=512,\n",
    "                 axial_shape=None,\n",
    "                 axial_emb_dims=None,\n",
    "                 pad_idx=None,\n",
    "                 prenorm=False,\n",
    "                 attn_bias=True):\n",
    "        store_attr('max_seq_len, n_layers, pad_idx')\n",
    "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                        axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        self.tfmr = TransformerEncoder(d_model, n_layers, heads, causal=causal, d_ff=d_ff, \n",
    "                                       attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
    "                                       prenorm=prenorm, attn_bias=attn_bias, final_norm=nn.LayerNorm)\n",
    "        self.proj = nn.Linear(d_model, vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.emb(x)\n",
    "        x = self.tfmr(x, mask=mask)\n",
    "        return self.proj(x)\n",
    "    \n",
    "    #TODO maybe refactor\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inp,\n",
    "                max_len=50,\n",
    "                temperature=1.,\n",
    "                method = 'top_k',\n",
    "                top_k = 20,\n",
    "                top_p = 0.9,\n",
    "                early_stopping=False, #need eos_idx to work\n",
    "                eos_idx=None):\n",
    "        self.to(inp.device) #TODO test for potential problems\n",
    "        self.eval()\n",
    "        thresh = top_k if method=='top_k' else top_p\n",
    "        sampler = _sampler[method]\n",
    "        inp = expand_dim1(inp)\n",
    "        b, t = inp.shape\n",
    "        out = inp\n",
    "        for _ in range(max_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "\n",
    "            logits = self(x)[:, -1, :]\n",
    "            if method == 'greedy':\n",
    "                sample = sampler(logits)\n",
    "            else:\n",
    "                filtered_logits = sampler(logits)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "                sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "\n",
    "            if early_stopping and (sample == eos_idx).all():\n",
    "                break\n",
    "        # out = out[:, t:]\n",
    "        return out\n",
    "\n",
    "    def store_attention(self, layer_ids=None):\n",
    "        #defaults to storing attention for all layers\n",
    "        layer_ids = default(layer_ids, list(range(self.n_layers)))\n",
    "        for module in self.children():\n",
    "            if issubclass(type(module), (TransformerEncoder, TransformerDecoder)):\n",
    "                for i, l in enumerate(module.layers):\n",
    "                    if i in layer_ids:\n",
    "                        for m in l.modules():\n",
    "                            if issubclass(type(m), (Attention)):\n",
    "                                m.store_attention = True\n",
    "    def get_attention_matrix(self):\n",
    "        res = []\n",
    "        for m in self.modules():\n",
    "            if issubclass(type(m), (Attention)):\n",
    "                attention = getattr(m, 'attention', None)\n",
    "                if attention is not None:\n",
    "                    res.append(attention)\n",
    "                # reset stored attention\n",
    "                m.attention = None\n",
    "                m.store_attention = False\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO test weight tying\n",
    "# Note on weight tying: it's done like here in fastai AWD_LSTM model\n",
    "# Lucidrains does it with custom MatrixMultiply module https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py#L106\n",
    "#TODO: update docstrings\n",
    "class TransformerEncDec(Module):\n",
    "    \"\"\"\n",
    "    Basic Transformer Encoder-Decoder model\n",
    "    Parameters:\n",
    "        * enc_vocab_sz: int - source vocab size \n",
    "        * dec_vocab_sz: int - target vocab size\n",
    "        * d_model: int - inner dimension of the model\n",
    "        * n_enc_layers: int (default: 6) \n",
    "        * n_dec_layers: int (default: 6) \n",
    "        * heads: int (default: 8)\n",
    "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
    "        * attn_dropout: float - attention dropout\n",
    "        * ff_dropout: float - feed-forward dropout\n",
    "        * emb_dropout: float - embedding dropout\n",
    "        * max_seq_len: int (default: 512)\n",
    "        * prenorm: bool - whether to use PreNorm or PostNorm\n",
    "        * attn_bias: bool - whether to allow biases in attention projection layers\n",
    "        * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are passed to \n",
    "                forward method will be used to generate padding masks\n",
    "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
    "        * shared_emb: bool - if True encoder and decoder will use shared embedding layer\n",
    "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
    "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
    "                max_seq_len\n",
    "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
    "    Inputs:\n",
    "        * src - source input ids, shape [bs, src_sl]\n",
    "        * tgt - target input ids, shape [bs, tgt_sl]\n",
    "        * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
    "        * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
    "    Returns:\n",
    "        * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 enc_vocab_sz, \n",
    "                 dec_vocab_sz, \n",
    "                 d_model, \n",
    "                 n_enc_layers=6, \n",
    "                 n_dec_layers=6, \n",
    "                 heads=8, \n",
    "                 d_ff=None,\n",
    "                 pad_idx=None, \n",
    "                 tie_weights=True,\n",
    "                 shared_emb = False,\n",
    "                 attn_dropout=0.1, \n",
    "                 ff_dropout=0.1, \n",
    "                 emb_dropout=0.1,\n",
    "                 prenorm=False, \n",
    "                 attn_bias=True,\n",
    "                 comb_attn=False, \n",
    "                 pos_enc='absolute', \n",
    "                 max_seq_len=512, \n",
    "                 axial_shape=None, \n",
    "                 axial_emb_dims=None):\n",
    "        store_attr('max_seq_len, n_enc_layers, n_dec_layers, pad_idx')\n",
    "        self.enc_emb = TransformerEmbedding(enc_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        if shared_emb:\n",
    "            assert (enc_vocab_sz == dec_vocab_sz), \"Encoder and decoder vocab size doesn't match\"\n",
    "            self.dec_emb = self.emc_emb\n",
    "        else:\n",
    "            self.dec_emb = TransformerEmbedding(dec_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
    "                                                axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
    "        \n",
    "        self.encoder = TransformerEncoder(d_model, n_enc_layers, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                          prenorm=prenorm, attn_bias=attn_bias, final_norm=nn.LayerNorm, causal=False)\n",
    "        self.decoder = TransformerDecoder(d_model, n_dec_layers, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
    "                                          prenorm=prenorm, comb_attn=comb_attn, attn_bias=attn_bias, final_norm=nn.LayerNorm)\n",
    "        self.proj = nn.Linear(d_model, dec_vocab_sz)\n",
    "        if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "        tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
    "        enc = self.encoder(self.enc_emb(src), mask=src_mask)\n",
    "        out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
    "        return self.proj(out)\n",
    "    \n",
    "    def get_padding_mask(self, x):\n",
    "        if self.pad_idx is None: return None\n",
    "        return (x != self.pad_idx)\n",
    "    \n",
    "    #TODO add beam search and refactor\n",
    "    @torch.no_grad()\n",
    "    def generate(self, src,\n",
    "                src_mask=None,\n",
    "                max_len=50,\n",
    "                temperature=1.,\n",
    "                method = 'top_k',\n",
    "                top_k = 20,\n",
    "                top_p = 0.9,\n",
    "                early_stopping=False,\n",
    "                bos_idx=2, # TODO change to match future usecases\n",
    "                eos_idx=None):\n",
    "        self.to(src.device) #TODO test for potential problems\n",
    "        self.eval()\n",
    "        thresh = top_k if method=='top_k' else top_p\n",
    "        sampler = _sampler[method]\n",
    "        src = expand_dim1(src)\n",
    "        bs = src.size(0)\n",
    "        inp = src.new_full((bs, 1), bos_idx) #start with bos tokens\n",
    "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
    "        enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
    "        out = inp\n",
    "        for _ in range(max_len):\n",
    "            x = out[:, -self.max_seq_len:]\n",
    "            dec = self.decoder(self.dec_emb(out), context=enc)\n",
    "            logits = self.proj(dec)[:, -1, :]\n",
    "            if method == 'greedy':\n",
    "                sample = sampler(logits)\n",
    "            else:\n",
    "                filtered_logits = sampler(logits, thresh)\n",
    "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
    "                sample = torch.multinomial(probs, 1)\n",
    "\n",
    "            out = torch.cat((out, sample), dim=-1)\n",
    "\n",
    "            if (early_stopping and \n",
    "                ((sample == eos_idx).all() or \n",
    "                (sample == self.pad_idx).all())):\n",
    "                break\n",
    "        #TODO mb output cleanup\n",
    "        return out\n",
    "\n",
    "    def store_attention(self, layer_ids=None, store_encoder=False, store_decoder=True):\n",
    "        #defaults to storing attention for all layers\n",
    "        layer_ids = default(layer_ids, list(range(self.n_enc_layers)))\n",
    "        for module in self.children():\n",
    "            if issubclass(type(module), TransformerEncoder) and store_encoder:\n",
    "                for i, l in enumerate(module.layers):\n",
    "                    if i in layer_ids:\n",
    "                        for m in l.modules():\n",
    "                            if issubclass(type(m), (Attention)):\n",
    "                                m.store_attention = True\n",
    "            elif issubclass(type(module), TransformerDecoder) and store_decoder:\n",
    "                for i, l in enumerate(module.layers):\n",
    "                    if i in layer_ids:\n",
    "                        for m in l.modules():\n",
    "                            if issubclass(type(m), (Attention)):\n",
    "                                m.store_attention = True\n",
    "    #TODO mb separate encoder and decoder attention\n",
    "    def get_attention_matrix(self, get_encoder=False, get_decoder=True):\n",
    "        res = []\n",
    "        if get_encoder:\n",
    "            for m in self.encoder.modules():\n",
    "                if issubclass(type(m), (Attention)):\n",
    "                    attention = getattr(m, 'attention', None)\n",
    "                    if attention is not None:\n",
    "                        res.append(attention)\n",
    "                    # reset stored attention\n",
    "                    m.attention = None\n",
    "                    m.store_attention = False\n",
    "        if get_decoder:\n",
    "            for m in self.decoder.modules():\n",
    "                if issubclass(type(m), (Attention)):\n",
    "                    attention = getattr(m, 'attention', None)\n",
    "                    if attention is not None:\n",
    "                        res.append(attention)\n",
    "                    # reset stored attention\n",
    "                    m.attention = None\n",
    "                    m.store_attention = False\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_layers.ipynb.\n",
      "No export destination, ignored:\n",
      "#export\n",
      "from fastai.basics import *\n",
      "from standard_transformer.layers import *\n",
      "No export destination, ignored:\n",
      "#export\n",
      "# generative helpers\n",
      "# credit https://github.com/huggingface/transformers/blob/a0c62d249303a68f5336e3f9a96ecf9241d7abbe/src/transformers/generation_logits_process.py\n",
      "def top_p_filter(logits, top_p=0.9):\n",
      "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
      "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
      "\n",
      "    sorted_indices_to_remove = cum_probs > top_p\n",
      "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
      "    sorted_indices_to_remove[..., 0] = 0\n",
      "    # if min_tokens_to_keep > 1:\n",
      "    #         # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
      "    #         sorted_indices_to_remove[..., : min_tokens_to_keep - 1] = 0\n",
      "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
      "    logits[indices_to_remove] = float('-inf')\n",
      "    return logits\n",
      "\n",
      "def top_k_filter(logits, top_k=20):\n",
      "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
      "    logits[indices_to_remove] = float('-inf')\n",
      "    return logits\n",
      "\n",
      "_sampler = {\n",
      "    'top_k':top_k_filter,\n",
      "    'top_p':top_p_filter,\n",
      "    'gready':lambda x: x.argmax(-1)\n",
      "}\n",
      "No export destination, ignored:\n",
      "#export\n",
      "class TransformerLM(Module):\n",
      "    \"\"\"\n",
      "    Basic Transformer for language modelling\n",
      "    Parameters:\n",
      "        * vocab_sz: int\n",
      "        * d_model: int - inner dimension of the model\n",
      "        * n_layers: int (default: 6) \n",
      "        * heads: int (default: 8)\n",
      "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
      "        * attn_dropout: float - attention dropout\n",
      "        * ff_dropout: float - feed-forward dropout\n",
      "        * emb_dropout: float - embedding dropout\n",
      "        * causal: bool (default: True) - if True does causal masking automatically\n",
      "        * max_seq_len: int (default: 512)\n",
      "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
      "        * prenorm: bool - wether to use PreNorm or PostNorm\n",
      "        * attn_bias: bool - wether to allow biases in attention projection layers\n",
      "        * pad_idx: int - padding token id, required for autogeneration of padding mask\n",
      "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
      "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
      "                max_seq_len\n",
      "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
      "    Inputs:\n",
      "        * x - input ids, shape [bs, sl]\n",
      "        * mask - optional boolean mask, shape [bs, sl]\n",
      "    Returns:\n",
      "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
      "    \"\"\"\n",
      "    def __init__(self, \n",
      "                 vocab_sz, \n",
      "                 d_model, \n",
      "                 n_layers=6,\n",
      "                 heads=8,\n",
      "                 d_ff=None,\n",
      "                 attn_dropout=0.1,\n",
      "                 ff_dropout=0.1,\n",
      "                 emb_dropout=0.1,\n",
      "                 tie_weights=True,\n",
      "                 causal=True,\n",
      "                 pos_enc='absolute',\n",
      "                 max_seq_len=512,\n",
      "                 axial_shape=None,\n",
      "                 axial_emb_dims=None,\n",
      "                 pad_idx=None,\n",
      "                 prenorm=False,\n",
      "                 attn_bias=True):\n",
      "        store_attr('max_seq_len, n_layers, pad_idx')\n",
      "        self.emb = TransformerEmbedding(vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
      "                                        axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
      "        self.tfmr = TransformerEncoder(d_model, n_layers, heads, causal=causal, d_ff=d_ff, \n",
      "                                       attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
      "                                       prenorm=prenorm, attn_bias=attn_bias, final_norm=nn.LayerNorm)\n",
      "        self.proj = nn.Linear(d_model, vocab_sz)\n",
      "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
      "        \n",
      "    def forward(self, x, mask=None):\n",
      "        x = self.emb(x)\n",
      "        x = self.tfmr(x, mask=mask)\n",
      "        return self.proj(x)\n",
      "    \n",
      "    #TODO maybe refactor\n",
      "    @torch.no_grad()\n",
      "    def generate(self, inp,\n",
      "                max_len=50,\n",
      "                temperature=1.,\n",
      "                method = 'top_k',\n",
      "                top_k = 20,\n",
      "                top_p = 0.9,\n",
      "                early_stopping=False, #need eos_idx to work\n",
      "                eos_idx=None):\n",
      "        self.to(inp.device) #TODO test for potential problems\n",
      "        self.eval()\n",
      "        thresh = top_k if method=='top_k' else top_p\n",
      "        sampler = _sampler[method]\n",
      "        inp = expand_dim1(inp)\n",
      "        b, t = inp.shape\n",
      "        out = inp\n",
      "        for _ in range(max_len):\n",
      "            x = out[:, -self.max_seq_len:]\n",
      "\n",
      "            logits = self(x)[:, -1, :]\n",
      "            if method == 'greedy':\n",
      "                sample = sampler(logits)\n",
      "            else:\n",
      "                filtered_logits = sampler(logits)\n",
      "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
      "                sample = torch.multinomial(probs, 1)\n",
      "\n",
      "            out = torch.cat((out, sample), dim=-1)\n",
      "\n",
      "            if early_stopping and (sample == eos_idx).all():\n",
      "                break\n",
      "        # out = out[:, t:]\n",
      "        return out\n",
      "\n",
      "    def store_attention(self, layer_ids=None):\n",
      "        #defaults to storing attention for all layers\n",
      "        layer_ids = default(layer_ids, list(range(self.n_layers)))\n",
      "        for module in self.children():\n",
      "            if issubclass(type(module), (TransformerEncoder, TransformerDecoder)):\n",
      "                for i, l in enumerate(module.layers):\n",
      "                    if i in layer_ids:\n",
      "                        for m in l.modules():\n",
      "                            if issubclass(type(m), (Attention)):\n",
      "                                m.store_attention = True\n",
      "    def get_attention_matrix(self):\n",
      "        res = []\n",
      "        for m in self.modules():\n",
      "            if issubclass(type(m), (Attention)):\n",
      "                attention = getattr(m, 'attention', None)\n",
      "                if attention is not None:\n",
      "                    res.append(attention)\n",
      "                # reset stored attention\n",
      "                m.attention = None\n",
      "                m.store_attention = False\n",
      "        return res\n",
      "No export destination, ignored:\n",
      "#export\n",
      "#TODO test weight tying\n",
      "# Note on weight tying: it's done like here in fastai AWD_LSTM model\n",
      "# Lucidrains does it with custom MatrixMultiply module https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py#L106\n",
      "#TODO: update docstrings\n",
      "class TransformerEncDec(Module):\n",
      "    \"\"\"\n",
      "    Basic Transformer Encoder-Decoder model\n",
      "    Parameters:\n",
      "        * enc_vocab_sz: int - source vocab size \n",
      "        * dec_vocab_sz: int - target vocab size\n",
      "        * d_model: int - inner dimension of the model\n",
      "        * n_enc_layers: int (default: 6) \n",
      "        * n_dec_layers: int (default: 6) \n",
      "        * heads: int (default: 8)\n",
      "        * d_ff: int - inner dimension of the pointwise FeedForward net, if None defaults to 4*d_model\n",
      "        * attn_dropout: float - attention dropout\n",
      "        * ff_dropout: float - feed-forward dropout\n",
      "        * emb_dropout: float - embedding dropout\n",
      "        * max_seq_len: int (default: 512)\n",
      "        * prenorm: bool - whether to use PreNorm or PostNorm\n",
      "        * attn_bias: bool - whether to allow biases in attention projection layers\n",
      "        * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are passed to \n",
      "                forward method will be used to generate padding masks\n",
      "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
      "        * shared_emb: bool - if True encoder and decoder will use shared embedding layer\n",
      "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
      "        * axial_shape: tuple - required if 'axial' positional encoding are used, should be factors of \n",
      "                max_seq_len\n",
      "        * axial_emb_dims: tuple - [optional] axial embedding components, should sum to d_model\n",
      "    Inputs:\n",
      "        * src - source input ids, shape [bs, src_sl]\n",
      "        * tgt - target input ids, shape [bs, tgt_sl]\n",
      "        * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
      "        * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
      "    Returns:\n",
      "        * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
      "    \"\"\"\n",
      "    def __init__(self, \n",
      "                 enc_vocab_sz, \n",
      "                 dec_vocab_sz, \n",
      "                 d_model, \n",
      "                 n_enc_layers=6, \n",
      "                 n_dec_layers=6, \n",
      "                 heads=8, \n",
      "                 d_ff=None,\n",
      "                 pad_idx=None, \n",
      "                 tie_weights=True,\n",
      "                 shared_emb = False,\n",
      "                 attn_dropout=0.1, \n",
      "                 ff_dropout=0.1, \n",
      "                 emb_dropout=0.1,\n",
      "                 prenorm=False, \n",
      "                 attn_bias=True,\n",
      "                 comb_attn=False, \n",
      "                 pos_enc='absolute', \n",
      "                 max_seq_len=512, \n",
      "                 axial_shape=None, \n",
      "                 axial_emb_dims=None):\n",
      "        store_attr('max_seq_len, n_enc_layers, n_dec_layers, pad_idx')\n",
      "        self.enc_emb = TransformerEmbedding(enc_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
      "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
      "        if shared_emb:\n",
      "            assert (enc_vocab_sz == dec_vocab_sz), \"Encoder and decoder vocab size doesn't match\"\n",
      "            self.dec_emb = self.emc_emb\n",
      "        else:\n",
      "            self.dec_emb = TransformerEmbedding(dec_vocab_sz, d_model, max_seq_len, dropout=emb_dropout, pos_enc=pos_enc,\n",
      "                                                axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
      "        \n",
      "        self.encoder = TransformerEncoder(d_model, n_enc_layers, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
      "                                          prenorm=prenorm, attn_bias=attn_bias, final_norm=nn.LayerNorm, causal=False)\n",
      "        self.decoder = TransformerDecoder(d_model, n_dec_layers, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, \n",
      "                                          prenorm=prenorm, comb_attn=comb_attn, attn_bias=attn_bias, final_norm=nn.LayerNorm)\n",
      "        self.proj = nn.Linear(d_model, dec_vocab_sz)\n",
      "        if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
      "\n",
      "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
      "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
      "        tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
      "        enc = self.encoder(self.enc_emb(src), mask=src_mask)\n",
      "        out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
      "        return self.proj(out)\n",
      "    \n",
      "    def get_padding_mask(self, x):\n",
      "        if self.pad_idx is None: return None\n",
      "        return (x != self.pad_idx)\n",
      "    \n",
      "    #TODO add beam search and refactor\n",
      "    @torch.no_grad()\n",
      "    def generate(self, src,\n",
      "                src_mask=None,\n",
      "                max_len=50,\n",
      "                temperature=1.,\n",
      "                method = 'top_k',\n",
      "                top_k = 20,\n",
      "                top_p = 0.9,\n",
      "                early_stopping=False,\n",
      "                bos_idx=2, # TODO change to match future usecases\n",
      "                eos_idx=None):\n",
      "        self.to(src.device) #TODO test for potential problems\n",
      "        self.eval()\n",
      "        thresh = top_k if method=='top_k' else top_p\n",
      "        sampler = _sampler[method]\n",
      "        src = expand_dim1(src)\n",
      "        bs = src.size(0)\n",
      "        inp = src.new_full((bs, 1), bos_idx) #start with bos tokens\n",
      "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
      "        enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
      "        out = inp\n",
      "        for _ in range(max_len):\n",
      "            x = out[:, -self.max_seq_len:]\n",
      "            dec = self.decoder(self.dec_emb(out), context=enc)\n",
      "            logits = self.proj(dec)[:, -1, :]\n",
      "            if method == 'greedy':\n",
      "                sample = sampler(logits)\n",
      "            else:\n",
      "                filtered_logits = sampler(logits, thresh)\n",
      "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
      "                sample = torch.multinomial(probs, 1)\n",
      "\n",
      "            out = torch.cat((out, sample), dim=-1)\n",
      "\n",
      "            if (early_stopping and \n",
      "                ((sample == eos_idx).all() or \n",
      "                (sample == self.pad_idx).all())):\n",
      "                break\n",
      "        #TODO mb output cleanup\n",
      "        return out\n",
      "\n",
      "    def store_attention(self, layer_ids=None, store_encoder=False, store_decoder=True):\n",
      "        #defaults to storing attention for all layers\n",
      "        layer_ids = default(layer_ids, list(range(self.n_enc_layers)))\n",
      "        for module in self.children():\n",
      "            if issubclass(type(module), TransformerEncoder) and store_encoder:\n",
      "                for i, l in enumerate(module.layers):\n",
      "                    if i in layer_ids:\n",
      "                        for m in l.modules():\n",
      "                            if issubclass(type(m), (Attention)):\n",
      "                                m.store_attention = True\n",
      "            elif issubclass(type(module), TransformerDecoder) and store_decoder:\n",
      "                for i, l in enumerate(module.layers):\n",
      "                    if i in layer_ids:\n",
      "                        for m in l.modules():\n",
      "                            if issubclass(type(m), (Attention)):\n",
      "                                m.store_attention = True\n",
      "    #TODO mb separate encoder and decoder attention\n",
      "    def get_attention_matrix(self, get_encoder=False, get_decoder=True):\n",
      "        res = []\n",
      "        if get_encoder:\n",
      "            for m in self.encoder.modules():\n",
      "                if issubclass(type(m), (Attention)):\n",
      "                    attention = getattr(m, 'attention', None)\n",
      "                    if attention is not None:\n",
      "                        res.append(attention)\n",
      "                    # reset stored attention\n",
      "                    m.attention = None\n",
      "                    m.store_attention = False\n",
      "        if get_decoder:\n",
      "            for m in self.decoder.modules():\n",
      "                if issubclass(type(m), (Attention)):\n",
      "                    attention = getattr(m, 'attention', None)\n",
      "                    if attention is not None:\n",
      "                        res.append(attention)\n",
      "                    # reset stored attention\n",
      "                    m.attention = None\n",
      "                    m.store_attention = False\n",
      "        return res\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f9b42ff186dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36mnotebook2script\u001b[0;34m(fname, silent, to_dict)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mod_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_notebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lib_path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36m_notebook2script\u001b[0;34m(fname, modules, silent, to_dict)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_from_future_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"'{f}'\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' +$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.8/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36m_add2all\u001b[0;34m(fname, names, line_width)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mtw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_long_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mre_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_re__all__def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mtext_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{text[start:end-1]}{'' if text[end-2]=='[' else ', '}{', '.join(names)}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_all\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv] *",
   "language": "python",
   "name": "conda-env-torchenv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
