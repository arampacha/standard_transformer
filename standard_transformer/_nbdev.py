# AUTOGENERATED BY NBDEV! DO NOT EDIT!

__all__ = ["index", "modules", "custom_doc_links", "git_url"]

index = {"exists": "01_layers.ipynb",
         "default": "01_layers.ipynb",
         "expand_dim1": "01_layers.ipynb",
         "Residual": "01_layers.ipynb",
         "PostNorm": "01_layers.ipynb",
         "PreNorm": "01_layers.ipynb",
         "FeedForward": "01_layers.ipynb",
         "MASK_VAL": "01_layers.ipynb",
         "Attention": "01_layers.ipynb",
         "AdditiveAttention": "01_layers.ipynb",
         "AttnInProj": "01_layers.ipynb",
         "ScaledDotProdAttention": "01_layers.ipynb",
         "TransformerEncoderBlock": "01_layers.ipynb",
         "TransformerEncoder": "01_layers.ipynb",
         "TransformerDecoderBlock": "01_layers.ipynb",
         "TransformerDecoderBlockV2": "01_layers.ipynb",
         "TransformerDecoder": "01_layers.ipynb",
         "AbsolutePositionalEmbedding": "01_layers.ipynb",
         "FixedPositionalEmbedding": "01_layers.ipynb",
         "TransformerEmbedding": "01_layers.ipynb",
         "top_p_filter": "02_models.ipynb",
         "top_k_filter": "02_models.ipynb",
         "sampler": "02_models.ipynb",
         "get_axial_dims": "02_models.ipynb",
         "LMMixin": "02_models.ipynb",
         "EncDecMixin": "02_models.ipynb",
         "TransformerLM": "02_models.ipynb",
         "Transformer": "02_models.ipynb"}

modules = ["layers.py",
           "models.py"]

doc_url = "https://arampacha.github.io/standard_transformer/"

git_url = "https://github.com/arampacha/standard_transformer/tree/master/"

def custom_doc_links(name): return None
